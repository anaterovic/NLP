{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from tuwnlpie import logger\n",
    "from tuwnlpie.milestone1.model import NBClassifier\n",
    "from tuwnlpie.milestone1.utils import read_food_disease_csv, split_data\n",
    "\n",
    "# from tuwnlpie.milestone2.model import TorchModel\n",
    "from tuwnlpie.milestone2.utils import TorchTrainer, read_and_prepare_data, TorchDataset, split_data_set, \\\n",
    "                                        length_longest_sentence, encodeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/holu/Documents/project-1div7\n",
      "## Reading in Data ##\n",
      "['term1' 'term2' 'sentence' 'is_cause' 'is_treat' 'tokens' 'tokens_stem'\n",
      " 'tokens_lemma']\n",
      "## Finished reading and preparing data ##\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/Users/holu/Documents/project-1div7/')\n",
    "\n",
    "train_data = './data/crowd_truth_combined.csv'\n",
    "\n",
    "feature_col = 'tokens_lemma' #['term1', 'term2', 'sentence', 'tokens', 'tokens_stem', 'tokens_lemma']\n",
    "label_cols = ['is_cause', 'is_treat']\n",
    "\n",
    "# readIn\n",
    "print(\"## Reading in Data ##\")\n",
    "data_frame = read_and_prepare_data(train_data, shall_sdp=False)\n",
    "data_frame = data_frame[['tokens_lemma', 'tokens', 'is_cause', 'is_treat']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/holu/Documents/project-1div7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk \n",
    "from nltk import RegexpTokenizer\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/Users/holu/Documents/project-1div7/')\n",
    "\n",
    "path = './data/crowd_truth_combined.csv'\n",
    "\n",
    "usedcols = ['sentence', 'term1', 'term2', 'is_cause', 'is_treat']\n",
    "df = pd.read_csv(\n",
    "    path,\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=usedcols)\n",
    "\n",
    "# Make case insensitive (no loss because emphasis on words does not play a role)\n",
    "df['sentence'] = df['sentence'].map(lambda x: x.lower())\n",
    "# Replace entities in sentence with placeholder tokens (may be useful for generalization when using n-grams)\n",
    "df['sentence'] = df.apply(lambda x: x['sentence'].replace(x['term1'].lower(), 'TERMONE').replace('TERMONEs', 'TERMONE'), axis=1)\n",
    "df['sentence'] = df.apply(lambda x: x['sentence'].replace(x['term2'].lower(), 'TERMTWO').replace('TERMTWOs', 'TERMTWO'), axis=1)\n",
    "\n",
    "df = df[df['sentence'].apply(lambda x: 'TERMONE' in x and 'TERMTWO' in x)]\n",
    "\n",
    "# Convert labels to right dtype\n",
    "df['is_cause'] = df['is_cause'].astype(float).astype(int)\n",
    "df['is_treat'] = df['is_treat'].astype(float).astype(int)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df['sentence'].apply(lambda x: tokenizer.tokenize(x))\n",
    "# Remove stop words and tokens with length smaller than 2 (i.e. punctuations)\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in nltk.corpus.stopwords.words('english') and len(token) > 1])\n",
    "# Perform stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "df['tokens_stem'] = df['tokens'].apply(lambda x: [porter.stem(token) for token in x])\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['tokens_lemma'] = df['tokens_stem'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(df['sentence'][0])\n",
    "def shortest_dep_path(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((\n",
    "                '{0}'.format(token.lemma_),\n",
    "                '{0}'.format(child.lemma_)))\n",
    "    graph = nx.Graph(edges)\n",
    "    entity1 = 'TERMONE'\n",
    "    entity2 = 'TERMTWO'\n",
    "    try:\n",
    "        return nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [x for x in tokens if x not in nltk.corpus.stopwords.words('english') and len(x) > 1]\n",
    "\n",
    "\n",
    "df['sdp_tokens_lemma'] = df['sentence'].apply(lambda x: remove_stop_words(shortest_dep_path(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['limit', 'data', 'suggest', 'child', 'mental', 'retard', 'termon', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
      "['termon', 'associ', 'difficult', 'behavior', 'termtwo', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
      "['term', 'termon', 'employ', 'indic', 'ataxia', 'due', 'termtwo']\n",
      "['non', 'hereditari', 'caus', 'termon', 'includ', 'termtwo', 'paraneoplast', 'termon', 'high', 'altitud', 'cerebr', 'oedema', 'coeliac', 'diseas', 'normal', 'pressur', 'hydrocephalu', 'cerebel']\n",
      "['disord', 'present', 'migratori', 'ture', 'termtwo', 'mani', 'featur', 'like', 'termon', 'skin', 'rash', 'gait', 'abnorm', 'skin', 'nodul']\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "X_loaded = data_frame[['tokens_lemma', 'tokens']].iloc[0:5]\n",
    "y_loaded = data_frame[['is_cause', 'is_treat']]\n",
    "# print(X_loaded)\n",
    "for sentence in X_loaded['tokens_lemma'].iloc[0:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8],\n",
       "        [ 4],\n",
       "        [10],\n",
       "        [ 4],\n",
       "        [10],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[18],\n",
       "        [17],\n",
       "        [10],\n",
       "        [17],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[17],\n",
       "        [ 7],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        [21],\n",
       "        [17],\n",
       "        [10],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[17],\n",
       "        [ 1],\n",
       "        [ 4],\n",
       "        [ 8],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[10],\n",
       "        [21],\n",
       "        [16],\n",
       "        [10],\n",
       "        [17],\n",
       "        [ 8],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[ 8],\n",
       "        [21],\n",
       "        [10],\n",
       "        [17],\n",
       "        [ 8],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[10],\n",
       "        [21],\n",
       "        [ 8],\n",
       "        [10],\n",
       "        [18],\n",
       "        [16],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[17],\n",
       "        [17],\n",
       "        [17],\n",
       "        [18],\n",
       "        [17],\n",
       "        [ 4],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[17],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        [ 8],\n",
       "        [21],\n",
       "        [17],\n",
       "        [17],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[18],\n",
       "        [21],\n",
       "        [17],\n",
       "        [10],\n",
       "        [ 8],\n",
       "        [ 7],\n",
       "        [17],\n",
       "        [10],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[18],\n",
       "        [ 8],\n",
       "        [18],\n",
       "        [18],\n",
       "        [21],\n",
       "        [ 8],\n",
       "        [10],\n",
       "        [ 4],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[10],\n",
       "        [21],\n",
       "        [ 8],\n",
       "        [10],\n",
       "        [10],\n",
       "        [16],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]]], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# X_loaded = df[['tokens_lemma', 'tokens']].iloc[0:1]\n",
    "\n",
    "# for sentence in X_loaded['tokens_lemma']:\n",
    "#     print(sentence)\n",
    "# for sentence in X_loaded['tokens']:\n",
    "#     print(sentence)\n",
    "\n",
    "lemmas = ['limit', 'data', 'suggest', 'child', 'mental', 'retard', 'termon', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
    "tokens = ['limited', 'data', 'suggest', 'children', 'mental', 'retardation', 'TERMONE', 'associated', 'aggression', 'destruction', 'property', 'TERMTWO']\n",
    "\n",
    "unique_words = set()\n",
    "longest_sentence = 0\n",
    "for sentence in tokens:\n",
    "    current_sentence = 0\n",
    "    for word in sentence:\n",
    "        current_sentence += 1\n",
    "        if word not in unique_words:\n",
    "            unique_words.add(word)\n",
    "        if current_sentence > longest_sentence:\n",
    "            longest_sentence = current_sentence\n",
    "\n",
    "X_tmp = []\n",
    "for sentence in lemmas:\n",
    "    sen_tmp = []\n",
    "    for token in sentence:\n",
    "        sen_tmp.append(one_hot(token, len(unique_words)))\n",
    "    X_tmp.append(sen_tmp)\n",
    "\n",
    "X_tmp = pad_sequences(X_tmp, longest_sentence, padding='post') \n",
    "# makes all sentences the same length by padding with preset value at the end\n",
    "X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_frame.columns.values)\n",
    "X_loaded = data_frame[['tokens_lemma', 'tokens']]\n",
    "y_loaded = data_frame[['is_cause', 'is_treat']]\n",
    "\n",
    "print(\"## Encoding ##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_lemma</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[limit, data, suggest, child, mental, retard, ...</td>\n",
       "      <td>[limited, data, suggest, children, mental, ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[termon, associ, difficult, behavior, termtwo,...</td>\n",
       "      <td>[TERMONEs, associated, difficult, behaviors, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[term, termon, employ, indic, ataxia, due, ter...</td>\n",
       "      <td>[term, TERMONE, employed, indicate, ataxia, du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[non, hereditari, caus, termon, includ, termtw...</td>\n",
       "      <td>[non, hereditary, causes, TERMONE, include, TE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[disord, present, migratori, ture, termtwo, ma...</td>\n",
       "      <td>[disorder, present, migratory, ture, TERMTWO, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7963</th>\n",
       "      <td>[61, year, old, man, termon, pd, develop, sudd...</td>\n",
       "      <td>[61, year, old, man, TERMONE, pd, developed, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7964</th>\n",
       "      <td>[success, treatment, patient, termon, termtwo,...</td>\n",
       "      <td>[successful, treatment, patient, TERMONE, TERM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>[five, 15, patient, receiv, termtwo, experi, t...</td>\n",
       "      <td>[five, 15, patients, receiving, TERMTWO, exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>[develop, antibodi, termtwo, seriou, complic, ...</td>\n",
       "      <td>[development, antibodies, TERMTWO, serious, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7967</th>\n",
       "      <td>[influenc, termtwo, termon, studi, doubl, blin...</td>\n",
       "      <td>[influence, TERMTWO, TERMONE, studied, double,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7821 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokens_lemma  \\\n",
       "0     [limit, data, suggest, child, mental, retard, ...   \n",
       "1     [termon, associ, difficult, behavior, termtwo,...   \n",
       "2     [term, termon, employ, indic, ataxia, due, ter...   \n",
       "3     [non, hereditari, caus, termon, includ, termtw...   \n",
       "4     [disord, present, migratori, ture, termtwo, ma...   \n",
       "...                                                 ...   \n",
       "7963  [61, year, old, man, termon, pd, develop, sudd...   \n",
       "7964  [success, treatment, patient, termon, termtwo,...   \n",
       "7965  [five, 15, patient, receiv, termtwo, experi, t...   \n",
       "7966  [develop, antibodi, termtwo, seriou, complic, ...   \n",
       "7967  [influenc, termtwo, termon, studi, doubl, blin...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [limited, data, suggest, children, mental, ret...  \n",
       "1     [TERMONEs, associated, difficult, behaviors, T...  \n",
       "2     [term, TERMONE, employed, indicate, ataxia, du...  \n",
       "3     [non, hereditary, causes, TERMONE, include, TE...  \n",
       "4     [disorder, present, migratory, ture, TERMTWO, ...  \n",
       "...                                                 ...  \n",
       "7963  [61, year, old, man, TERMONE, pd, developed, s...  \n",
       "7964  [successful, treatment, patient, TERMONE, TERM...  \n",
       "7965  [five, 15, patients, receiving, TERMTWO, exper...  \n",
       "7966  [development, antibodies, TERMTWO, serious, co...  \n",
       "7967  [influence, TERMTWO, TERMONE, studied, double,...  \n",
       "\n",
       "[7821 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_loaded\n",
    "y = y_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['limit', 'data', 'suggest', 'child', 'mental', 'retard', 'termon', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
    "['limit', 'data', 'suggest', 'child', 'mental', 'retard', 'term_on', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
    "['termon', 'associ', 'difficult', 'behavior', 'termtwo', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
    "['termon', 'associ', 'difficult', 'behavior', 'termtwo', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
    "['term', 'termon', 'employ', 'indic', 'ataxia', 'due', 'termtwo']\n",
    "['term', 'termon', 'employ', 'indic', 'ataxia', 'due', 'termtwo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encodeX(X_loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10112],\n",
       "        [ 8492],\n",
       "        [ 6924],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 5046],\n",
       "        [10226],\n",
       "        [ 5531],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 7782],\n",
       "        [ 5046],\n",
       "        [ 1165],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9782],\n",
       "        [ 5624],\n",
       "        [  652],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 1678],\n",
       "        [10165],\n",
       "        [ 4983],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 6059],\n",
       "        [ 4983],\n",
       "        [ 5046],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10112],\n",
       "        [ 8492],\n",
       "        [ 6924],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 5046],\n",
       "        [10226],\n",
       "        [ 5531],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 7782],\n",
       "        [ 5046],\n",
       "        [ 1165],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9782],\n",
       "        [ 5624],\n",
       "        [  652],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 1678],\n",
       "        [10165],\n",
       "        [ 4983],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]],\n",
       "\n",
       "       [[ 6059],\n",
       "        [ 4983],\n",
       "        [ 5046],\n",
       "        ...,\n",
       "        [    0],\n",
       "        [    0],\n",
       "        [    0]]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding -> WORKING!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7821, 110, 1)\n",
      "## Split ##\n",
      "## Creating Data-Loaders ##\n"
     ]
    }
   ],
   "source": [
    "# X = X['tokens_lemma']\n",
    "\n",
    "print(X.shape)\n",
    "print(\"## Split ##\")\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = split_data_set(X, y,  size=0.8, random_state=1)\n",
    "X_test, X_val, y_test, y_val = split_data_set(X_test, y_test, size=0.5, random_state=1)\n",
    "\n",
    "\n",
    "print(\"## Creating Data-Loaders ##\")\n",
    "# Data Loaders\n",
    "train_loader = TorchDataset(X_train,y_train)\n",
    "val_loader = TorchDataset(X_val,y_val)\n",
    "\n",
    "dataloaders = { \n",
    "    'train': train_loader.get_dataloader(batch_size=256, shuffle=True), \n",
    "    'val': val_loader.get_dataloader(batch_size=128, shuffle=False)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, matmul\n",
    "from torch.nn.functional import softmax\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_position_embedding import PositionEmbedding\n",
    "\n",
    "\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "class TorchModel(LightningModule):\n",
    "    def __init__(self, learning_rate=1e-2) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate')\n",
    "\n",
    "        self.wordEmbeddings = nn.Embedding(11212,110)\n",
    "        self.positionEmbeddings = nn.Embedding(110,40)\n",
    "        # self.positionEmbeddings = PositionEmbedding(num_embeddings=11212, embedding_dim=110, mode=PositionEmbedding.MODE_ADD)\n",
    "        self.transformerLayer = nn.TransformerEncoderLayer(150,15) #this transofrmer contains muti head attention\n",
    "        self.linear1 = nn.Linear(150, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        self.linear3 = nn.Linear(110,  16)\n",
    "        self.linear4 = nn.Linear(16, 2)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        positions = (torch.arange(0,110).reshape(1,110) + torch.zeros(x.shape[0],110)).to(device)\n",
    "        sentence = torch.cat((self.wordEmbeddings(x.long()).squeeze(2),self.positionEmbeddings(positions.long())),axis=2)\n",
    "        attended = self.transformerLayer(sentence)\n",
    "        linear1 = F.relu(self.linear1(attended))\n",
    "        linear2 = torch.sigmoid(self.linear2(linear1))\n",
    "        linear2 = linear2.view(-1,110) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
    "        linear3 = F.relu(self.linear3(linear2))\n",
    "        out = torch.sigmoid(self.linear4(linear3))\n",
    "        return out\n",
    "    \n",
    "    def _loss_fn(self, out, y):\n",
    "        loss = F.binary_cross_entropy(out, y) # Multiclass classification\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        out = out.squeeze()\n",
    "        loss = self._loss_fn(out, y.float())\n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)      \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        print(\"TEST DATA\")\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            out = self(x)\n",
    "            out = out.squeeze()\n",
    "            loss = self._loss_fn(out, y.float())\n",
    "            report = classification_report(np.argmax(y, axis=1),np.argmax(out, axis=1),target_names=['is_cause', 'is_treat'])\n",
    "            print(report)\n",
    "            \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            out = self(x)\n",
    "            out = out.squeeze()\n",
    "            loss = self._loss_fn(out, y.float())\n",
    "            self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adagrad(\n",
    "            self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer():\n",
    "    def __init__(self, model, name, dirpath, dataloaders, max_epochs=50) -> None:\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.dirpath = dirpath\n",
    "        self.max_epochs = max_epochs\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def run(self):\n",
    "        logger = TensorBoardLogger(f\"{self.dirpath}/tensorboard\", name=self.name)\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(dirpath=Path(self.dirpath, self.name), monitor=\"val_loss\"),\n",
    "            EarlyStopping(monitor='loss')\n",
    "            ]\n",
    "        trainer = Trainer(deterministic=True, logger=logger, callbacks=callbacks, max_epochs=self.max_epochs)\n",
    "        trainer.fit(self.model, self.dataloaders['train'], self.dataloaders['val'])\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holu/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "model = TorchModel()\n",
    "\n",
    "trainer = TorchTrainer(model, 'test', \"../tuwnlpie/milestone2/lightning_logs/version_0/checkpoints/\" , dataloaders, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 13:34:52,169 : setup (162) - INFO - GPU available: True (mps), used: False\n",
      "2023-01-12 13:34:52,214 : setup (165) - INFO - TPU available: False, using: 0 TPU cores\n",
      "2023-01-12 13:34:52,215 : setup (168) - INFO - IPU available: False, using: 0 IPUs\n",
      "2023-01-12 13:34:52,215 : setup (171) - INFO - HPU available: False, using: 0 HPUs\n",
      "/Users/holu/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "2023-01-12 13:34:52,217 : tensorboard (325) - WARNING - Missing logger folder: ../tuwnlpie/milestone2/lightning_logs/version_0/checkpoints//tensorboard/test\n",
      "2023-01-12 13:34:52,220 : model_summary (83) - INFO - \n",
      "  | Name               | Type                    | Params\n",
      "---------------------------------------------------------------\n",
      "0 | wordEmbeddings     | Embedding               | 1.2 M \n",
      "1 | positionEmbeddings | Embedding               | 4.4 K \n",
      "2 | transformerLayer   | TransformerEncoderLayer | 707 K \n",
      "3 | linear1            | Linear                  | 9.7 K \n",
      "4 | linear2            | Linear                  | 65    \n",
      "5 | linear3            | Linear                  | 1.8 K \n",
      "6 | linear4            | Linear                  | 34    \n",
      "---------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.828     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b118f8ccad449ea1bd58d483b83d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holu/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m the_trainer \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn [10], line 16\u001b[0m, in \u001b[0;36mTorchTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     ModelCheckpoint(dirpath\u001b[39m=\u001b[39mPath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirpath, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname), monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     13\u001b[0m     EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m     ]\n\u001b[1;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, logger\u001b[39m=\u001b[39mlogger, callbacks\u001b[39m=\u001b[39mcallbacks, max_epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs)\n\u001b[0;32m---> 16\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloaders[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloaders[\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 603\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    605\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:645\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    638\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    641\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m )\n\u001b[0;32m--> 645\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    647\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    648\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1098\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1098\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1100\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1177\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1190\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1189\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1262\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1262\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1266\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:121\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    120\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[0;32m--> 121\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    266\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/tuwnlpie-0.0.1-py3.8.egg/tuwnlpie/milestone2/utils.py:161\u001b[0m, in \u001b[0;36mTorchDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m    160\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[idx]\n\u001b[0;32m--> 161\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[idx]\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39mtensor(x)\u001b[39m.\u001b[39mfloat(), torch\u001b[39m.\u001b[39mtensor(y)\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "the_trainer = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da1dcd46b6553758266418fe5890fa8a3c5a00e3ea8225bcde0d5076085a934b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
