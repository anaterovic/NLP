{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, matmul\n",
    "from torch.nn.functional import softmax\n",
    "from torchmetrics import Accuracy\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\72095722.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df_caus.append(df_treat, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_cause</th>\n",
       "      <th>is_treat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUTISM</td>\n",
       "      <td>TANTRUM</td>\n",
       "      <td>The limited data suggest that, in children wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLEEP PROBLEM</td>\n",
       "      <td>FAMILY STRESS</td>\n",
       "      <td>SLEEP PROBLEMs are associated with difficult b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CEREBELLAR ATAXIA</td>\n",
       "      <td>DYSFUNCTION OF THE CEREBELLUM</td>\n",
       "      <td>The term CEREBELLAR ATAXIA is employed to indi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CEREBELLAR DEGENERATION</td>\n",
       "      <td>CHRONIC ETHANOL ABUSE</td>\n",
       "      <td>Non hereditary causes of cerebellar degenerati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HEART PROBLEM</td>\n",
       "      <td>ARTHRITIS</td>\n",
       "      <td>The disorder can present with a migratory ture...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7963</th>\n",
       "      <td>PARKINSON'S DISEASE</td>\n",
       "      <td>AMANTADINE</td>\n",
       "      <td>A 61 year old man with PARKINSON'S DISEASE (PD...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7964</th>\n",
       "      <td>DEPRESSION</td>\n",
       "      <td>IMIPRAMINE</td>\n",
       "      <td>With successful treatment of the patient's dep...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>ANGI</td>\n",
       "      <td>BEPRIDIL</td>\n",
       "      <td>Five of 15 patients receiving bepridil did not...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>HEMOPHILIA A</td>\n",
       "      <td>FACTOR VIII</td>\n",
       "      <td>The development of antibodies to factor VIII i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7967</th>\n",
       "      <td>FOOD STIMULATED GASTRO OESOPHAGEAL REFLUX MECH...</td>\n",
       "      <td>CISAPRIDE</td>\n",
       "      <td>The influence of CISAPRIDE on FOOD STIMULATED ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7968 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  term1  \\\n",
       "0                                                AUTISM   \n",
       "1                                         SLEEP PROBLEM   \n",
       "2                                     CEREBELLAR ATAXIA   \n",
       "3                               CEREBELLAR DEGENERATION   \n",
       "4                                         HEART PROBLEM   \n",
       "...                                                 ...   \n",
       "7963                                PARKINSON'S DISEASE   \n",
       "7964                                         DEPRESSION   \n",
       "7965                                               ANGI   \n",
       "7966                                       HEMOPHILIA A   \n",
       "7967  FOOD STIMULATED GASTRO OESOPHAGEAL REFLUX MECH...   \n",
       "\n",
       "                              term2  \\\n",
       "0                           TANTRUM   \n",
       "1                     FAMILY STRESS   \n",
       "2     DYSFUNCTION OF THE CEREBELLUM   \n",
       "3             CHRONIC ETHANOL ABUSE   \n",
       "4                         ARTHRITIS   \n",
       "...                             ...   \n",
       "7963                     AMANTADINE   \n",
       "7964                     IMIPRAMINE   \n",
       "7965                       BEPRIDIL   \n",
       "7966                    FACTOR VIII   \n",
       "7967                      CISAPRIDE   \n",
       "\n",
       "                                               sentence  is_cause  is_treat  \n",
       "0     The limited data suggest that, in children wit...         1         0  \n",
       "1     SLEEP PROBLEMs are associated with difficult b...         1         0  \n",
       "2     The term CEREBELLAR ATAXIA is employed to indi...         1         0  \n",
       "3     Non hereditary causes of cerebellar degenerati...         1         0  \n",
       "4     The disorder can present with a migratory ture...         1         0  \n",
       "...                                                 ...       ...       ...  \n",
       "7963  A 61 year old man with PARKINSON'S DISEASE (PD...         0         1  \n",
       "7964  With successful treatment of the patient's dep...         0         1  \n",
       "7965  Five of 15 patients receiving bepridil did not...         0         1  \n",
       "7966  The development of antibodies to factor VIII i...         0         1  \n",
       "7967  The influence of CISAPRIDE on FOOD STIMULATED ...         0         1  \n",
       "\n",
       "[7968 rows x 5 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "usedcols = ['sentence', 'term1', 'term2']\n",
    "\n",
    "df_caus = pd.read_csv(\n",
    "    Path('..', 'data', 'crowd_truth_cause.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=usedcols\n",
    ")\n",
    "df_caus[\"is_cause\"] = 1\n",
    "df_caus[\"is_treat\"] = 0\n",
    "df_treat = pd.read_csv(\n",
    "    Path('..', 'data', 'crowd_truth_treat.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=usedcols\n",
    ")\n",
    "df_treat[\"is_treat\"] = 1\n",
    "df_treat[\"is_cause\"] = 0\n",
    "df = df_caus.append(df_treat, ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the limited data suggest that, in children with mental retardation, TERMONE is associated with aggression, destruction of property, and TERMTWO.\n",
      "TERMONE are associated with difficult behaviors and TERMTWO, and are often a focus of clinical attention over and above the primary asd diagnosis.\n",
      "the term TERMONE is employed to indicate ataxia that is due to TERMTWO\n",
      "non hereditary causes of TERMONE include TERMTWO, paraneoplastic TERMONE, high altitude cerebral oedema, coeliac disease, normal pressure hydrocephalus and cerebellitis.\n",
      "the disorder can present with a migratory ture of TERMTWO with many other features like TERMONE, skin rash, gait abnormality and skin nodules.\n",
      "Number of docs: 7821\n"
     ]
    }
   ],
   "source": [
    "# Make case insensitive (no loss because emphasis on words does not play a role)\n",
    "df['sentence'] = df['sentence'].map(lambda x: x.lower())\n",
    "\n",
    "# Replace entities in sentence with placeholder tokens (may be useful for generalization when using n-grams)\n",
    "df['sentence'] = df.apply(lambda x: x['sentence'].replace(x['term1'].lower(), 'TERMONE').replace('TERMONEs', 'TERMONE'), axis=1)\n",
    "df['sentence'] = df.apply(lambda x: x['sentence'].replace(x['term2'].lower(), 'TERMTWO').replace('TERMTWOs', 'TERMTWO'), axis=1)\n",
    "\n",
    "for i in range(5):\n",
    "    print(df['sentence'][i])\n",
    "\n",
    "df = df[df['sentence'].apply(lambda x: 'TERMONE' in x and 'TERMTWO' in x)]\n",
    "\n",
    "print(f\"Number of docs: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\892057628.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_cause'] = df['is_cause'].astype(float).astype(int)\n",
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\892057628.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_treat'] = df['is_treat'].astype(float).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_cause</th>\n",
       "      <th>is_treat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_cause  is_treat\n",
       "0         1         0\n",
       "1         1         0\n",
       "2         1         0\n",
       "3         1         0\n",
       "4         1         0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to right dtype\n",
    "label_cols = ['is_cause', 'is_treat']\n",
    "df['is_cause'] = df['is_cause'].astype(float).astype(int)\n",
    "df['is_treat'] = df['is_treat'].astype(float).astype(int)\n",
    "df[label_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\219313641.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['sentence'].apply(lambda x: tokenizer.tokenize(x))\n",
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\219313641.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in nltk.corpus.stopwords.words('english') and len(token) > 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['limit', 'data', 'suggest', 'children', 'mental', 'retard', 'termon', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
      "['termon', 'associ', 'difficult', 'behavior', 'termtwo', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
      "['term', 'termon', 'employ', 'indic', 'ataxia', 'due', 'termtwo']\n",
      "['non', 'hereditari', 'caus', 'termon', 'includ', 'termtwo', 'paraneoplast', 'termon', 'high', 'altitud', 'cerebr', 'oedema', 'coeliac', 'diseas', 'normal', 'pressur', 'hydrocephalu', 'cerebel']\n",
      "['disord', 'present', 'migratori', 'ture', 'termtwo', 'mani', 'featur', 'like', 'termon', 'skin', 'rash', 'gait', 'abnorm', 'skin', 'nodul']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\219313641.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_stem'] = df['tokens'].apply(lambda x: [porter.stem(token) for token in x])\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import RegexpTokenizer\n",
    "nltk.download('punkt') # for tokanization\n",
    "nltk.download('stopwords') # for stopword removal\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df['sentence'].apply(lambda x: tokenizer.tokenize(x))\n",
    "# Remove stop words and tokens with length smaller than 2 (i.e. punctuations)\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in nltk.corpus.stopwords.words('english') and len(token) > 1])\n",
    "# Perform stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "df['tokens_stem'] = df['tokens'].apply(lambda x: [porter.stem(token) for token in x])\n",
    "for i in range(5):\n",
    "    print(df['tokens_stem'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['limit', 'data', 'suggest', 'child', 'mental', 'retard', 'termon', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
      "['termon', 'associ', 'difficult', 'behavior', 'termtwo', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
      "['term', 'termon', 'employ', 'indic', 'ataxia', 'due', 'termtwo']\n",
      "['non', 'hereditari', 'caus', 'termon', 'includ', 'termtwo', 'paraneoplast', 'termon', 'high', 'altitud', 'cerebr', 'oedema', 'coeliac', 'diseas', 'normal', 'pressur', 'hydrocephalu', 'cerebel']\n",
      "['disord', 'present', 'migratori', 'ture', 'termtwo', 'mani', 'featur', 'like', 'termon', 'skin', 'rash', 'gait', 'abnorm', 'skin', 'nodul']\n",
      "['featur', 'termtwo', 'includ', 'skin', 'rash', 'extrem', 'photosensit', 'hair', 'loss', 'kidney', 'problem', 'emotiol', 'labil', 'lung', 'fibrosi', 'termon']\n",
      "['mani', 'individu', 'termtwo', 'also', 'suffer', 'termon', 'high', 'cholesterol', 'heart', 'diseas']\n",
      "['traditiolli', 'termon', 'suggest', 'total', 'impair', 'languag', 'abil', 'termtwo', 'degre', 'impair', 'le', 'total']\n",
      "['termon', 'may', 'co', 'occur', 'termtwo', 'dysarthria', 'apraxia', 'speech', 'also', 'result', 'brain', 'damag']\n",
      "['main', 'use', 'treatment', 'hyperkinet', 'movement', 'disord', 'huntington', 'diseas', 'termtwo', 'rather', 'condit', 'termon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\419731317.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_lemma'] = df['tokens_stem'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n"
     ]
    }
   ],
   "source": [
    "# Dependencies for WorNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['tokens_lemma'] = df['tokens_stem'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "for i in range(10):\n",
    "    print(df['tokens_lemma'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "the limited data suggest that, in children with mental retardation, TERMONE is associated with aggression, destruction of property, and TERMTWO.\n",
      "['limit', 'data', 'suggest', 'child', 'mental', 'retard', 'termon', 'associ', 'aggress', 'destruct', 'properti', 'termtwo']\n",
      "[]\n",
      "Index: 1\n",
      "TERMONE are associated with difficult behaviors and TERMTWO, and are often a focus of clinical attention over and above the primary asd diagnosis.\n",
      "['termon', 'associ', 'difficult', 'behavior', 'termtwo', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
      "[]\n",
      "Index: 2\n",
      "the term TERMONE is employed to indicate ataxia that is due to TERMTWO\n",
      "['term', 'termon', 'employ', 'indic', 'ataxia', 'due', 'termtwo']\n",
      "[]\n",
      "Index: 3\n",
      "non hereditary causes of TERMONE include TERMTWO, paraneoplastic TERMONE, high altitude cerebral oedema, coeliac disease, normal pressure hydrocephalus and cerebellitis.\n",
      "['non', 'hereditari', 'caus', 'termon', 'includ', 'termtwo', 'paraneoplast', 'termon', 'high', 'altitud', 'cerebr', 'oedema', 'coeliac', 'diseas', 'normal', 'pressur', 'hydrocephalu', 'cerebel']\n",
      "[]\n",
      "Index: 4\n",
      "the disorder can present with a migratory ture of TERMTWO with many other features like TERMONE, skin rash, gait abnormality and skin nodules.\n",
      "['disord', 'present', 'migratori', 'ture', 'termtwo', 'mani', 'featur', 'like', 'termon', 'skin', 'rash', 'gait', 'abnorm', 'skin', 'nodul']\n",
      "[]\n",
      "Index: 5\n",
      "other features of TERMTWO include a skin rash, extreme photosensitivity, hair loss, kidney problems, emotiol lability, lung fibrosis and TERMONE\n",
      "['featur', 'termtwo', 'includ', 'skin', 'rash', 'extrem', 'photosensit', 'hair', 'loss', 'kidney', 'problem', 'emotiol', 'labil', 'lung', 'fibrosi', 'termon']\n",
      "[]\n",
      "Index: 6\n",
      "many individuals who have TERMTWO also suffer from TERMONE, high cholesterol or have heart disease.\n",
      "['mani', 'individu', 'termtwo', 'also', 'suffer', 'termon', 'high', 'cholesterol', 'heart', 'diseas']\n",
      "[]\n",
      "Index: 7\n",
      "traditiolly, TERMONE suggests the total impairment of language ability, and TERMTWO a degree of impairment less than total.\n",
      "['traditiolli', 'termon', 'suggest', 'total', 'impair', 'languag', 'abil', 'termtwo', 'degre', 'impair', 'le', 'total']\n",
      "[]\n",
      "Index: 8\n",
      "TERMONE may co-occur with TERMTWO such as dysarthria or apraxia of speech, which also result from brain damage.\n",
      "['termon', 'may', 'co', 'occur', 'termtwo', 'dysarthria', 'apraxia', 'speech', 'also', 'result', 'brain', 'damag']\n",
      "[]\n",
      "Index: 9\n",
      "its main usefulness is the treatment of hyperkinetic movement disorders such as huntington's disease and TERMTWO, rather than for conditions such as TERMONE.\n",
      "['main', 'use', 'treatment', 'hyperkinet', 'movement', 'disord', 'huntington', 'diseas', 'termtwo', 'rather', 'condit', 'termon']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\AppData\\Local\\Temp\\ipykernel_10384\\145915365.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sdp_tokens_lemma'] = df['sentence'].apply(lambda x: remove_stop_words(shortest_dep_path(x)))\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "doc = nlp(df['sentence'][0])\n",
    "\n",
    "def shortest_dep_path(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((\n",
    "                '{0}'.format(token.lemma_),\n",
    "                '{0}'.format(child.lemma_)))\n",
    "    graph = nx.Graph(edges)\n",
    "    entity1 = 'termon'\n",
    "    entity2 = 'termtwo'\n",
    "    try:\n",
    "        return nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [x for x in tokens if x not in nltk.corpus.stopwords.words('english') and len(x) > 1]\n",
    "\n",
    "df['sdp_tokens_lemma'] = df['sentence'].apply(lambda x: remove_stop_words(shortest_dep_path(x)))\n",
    "for i in range(10):\n",
    "    print(f\"Index: {i}\")\n",
    "    print(df['sentence'][i])\n",
    "    print(df['tokens_lemma'][i])\n",
    "    print(df['sdp_tokens_lemma'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['term1', 'term2', 'sentence', 'is_cause', 'is_treat', 'tokens',\n",
       "       'tokens_stem', 'tokens_lemma', 'sdp_tokens_lemma'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11212\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "longest_sentence = 0\n",
    "for sentence in df['tokens']:\n",
    "    current_sentence = 0\n",
    "    for word in sentence:\n",
    "        current_sentence += 1\n",
    "        if word not in unique_words:\n",
    "            unique_words.add(word)\n",
    "        if current_sentence > longest_sentence:\n",
    "            longest_sentence = current_sentence\n",
    "print(len(unique_words))\n",
    "print(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tokens_lemma'].to_numpy()\n",
    "y = df[['is_cause', 'is_treat']].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "import copy\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_tmp = []\n",
    "for sentence in df['tokens_lemma']:\n",
    "    sen_tmp = []\n",
    "    for token in sentence:\n",
    "        sen_tmp.append(one_hot(token, len(unique_words)))\n",
    "    X_tmp.append(sen_tmp)\n",
    "\n",
    "X_tmp = pad_sequences(X_tmp, longest_sentence, padding='post')\n",
    "\n",
    "X = copy.deepcopy(X_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.8, random_state=1)\n",
    "X_test, X_val, y_test, y_val= train_test_split(X_test, y_test, test_size=0.5, random_state=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def get_x(self):\n",
    "        return self.x\n",
    "\n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return (torch.tensor(x).float(), torch.tensor(y).long())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def get_dataloader(self, batch_size=128, num_workers=0, shuffle=False):\n",
    "        return DataLoader(self, batch_size=batch_size, drop_last=True, pin_memory=True, num_workers=num_workers, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = TorchDataset(X_train,y_train)\n",
    "X_test = TorchDataset(X_test, y_test)\n",
    "X_val = TorchDataset(X_val, y_val)\n",
    "\n",
    "dataloaders = { 'train': X_train.get_dataloader(batch_size=256, shuffle=True), 'test': X_test.get_dataloader(batch_size=128, shuffle=False), 'val': X_val.get_dataloader(batch_size=128, shuffle=False) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on read papers we will try to implement a model that uses multi head attention and positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "from torch_position_embedding import PositionEmbedding\n",
    "\n",
    "class TorchModel(LightningModule):\n",
    "    def __init__(self, learning_rate=1e-2) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('learning_rate')\n",
    "\n",
    "        self.wordEmbeddings = nn.Embedding(11212,110)\n",
    "        self.positionEmbeddings = nn.Embedding(110,40)\n",
    "        # self.positionEmbeddings = PositionEmbedding(num_embeddings=11212, embedding_dim=110, mode=PositionEmbedding.MODE_ADD)\n",
    "        self.transformerLayer = nn.TransformerEncoderLayer(150,15) #this transofrmer contains muti head attention\n",
    "        self.linear1 = nn.Linear(150, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        self.linear3 = nn.Linear(110,  16)\n",
    "        self.linear4 = nn.Linear(16, 2)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        positions = (torch.arange(0,110).reshape(1,110) + torch.zeros(x.shape[0],110)).to(device)\n",
    "        sentence = torch.cat((self.wordEmbeddings(x.long()).squeeze(2),self.positionEmbeddings(positions.long())),axis=2)\n",
    "        attended = self.transformerLayer(sentence)\n",
    "        linear1 = F.relu(self.linear1(attended))\n",
    "        linear2 = torch.sigmoid(self.linear2(linear1))\n",
    "        linear2 = linear2.view(-1,110) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
    "        linear3 = F.relu(self.linear3(linear2))\n",
    "        out = torch.sigmoid(self.linear4(linear3))\n",
    "        return out\n",
    "    \n",
    "    def _loss_fn(self, out, y):\n",
    "        loss = F.binary_cross_entropy(out, y)# Multiclass classification\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        out = out.squeeze()\n",
    "        loss = self._loss_fn(out, y.float())\n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)      \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        print(\"TEST DATA\")\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            out = self(x)\n",
    "            out = out.squeeze()\n",
    "            loss = self._loss_fn(out, y.float())\n",
    "            report = classification_report(np.argmax(y, axis=1),np.argmax(out, axis=1),target_names=['is_cause', 'is_treat'])\n",
    "            print(report)\n",
    "            \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            out = self(x)\n",
    "            out = out.squeeze()\n",
    "            loss = self._loss_fn(out, y.float())\n",
    "            self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adagrad(\n",
    "            self.parameters(), lr=self.hparams.learning_rate)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer():\n",
    "    def __init__(self, model, name, dirpath, dataloaders, max_epochs=50) -> None:\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.dirpath = dirpath\n",
    "        self.max_epochs = max_epochs\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def run(self):\n",
    "        logger = TensorBoardLogger(f\"{self.dirpath}/tensorboard\", name=self.name)\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(dirpath=Path(self.dirpath, self.name), monitor=\"val_loss\"),\n",
    "            EarlyStopping(monitor='loss')\n",
    "            ]\n",
    "        trainer = Trainer(deterministic=True, logger=logger, callbacks=callbacks, max_epochs=self.max_epochs)\n",
    "        trainer.fit(self.model, self.dataloaders['train'], self.dataloaders['val'])\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "model = TorchModel()\n",
    "\n",
    "trainer = TorchTrainer(model, 'test', \"../tuwnlpie/milestone2/lightning_logs/version_0/checkpoints/\" , dataloaders, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Ana\\Desktop\\FER\\dip\\tu_wien\\nlp\\project-1div7\\tuwnlpie\\milestone2\\lightning_logs\\version_0\\checkpoints\\test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name               | Type                    | Params\n",
      "---------------------------------------------------------------\n",
      "0 | wordEmbeddings     | Embedding               | 1.2 M \n",
      "1 | positionEmbeddings | Embedding               | 4.4 K \n",
      "2 | transformerLayer   | TransformerEncoderLayer | 707 K \n",
      "3 | linear1            | Linear                  | 9.7 K \n",
      "4 | linear2            | Linear                  | 65    \n",
      "5 | linear3            | Linear                  | 1.8 K \n",
      "6 | linear4            | Linear                  | 34    \n",
      "---------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.828     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1558: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 30/30 [00:41<00:00,  1.38s/it, loss=0.686, v_num=1, loss_step=0.683, val_loss=0.700, loss_epoch=0.680]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 30/30 [00:41<00:00,  1.38s/it, loss=0.686, v_num=1, loss_step=0.683, val_loss=0.700, loss_epoch=0.680]\n"
     ]
    }
   ],
   "source": [
    "the_trainer = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.39      0.47      0.43        59\n",
      "    is_treat       0.46      0.38      0.41        69\n",
      "\n",
      "    accuracy                           0.42       128\n",
      "   macro avg       0.43      0.43      0.42       128\n",
      "weighted avg       0.43      0.42      0.42       128\n",
      "\n",
      "Testing DataLoader 0:   4%|▍         | 1/24 [00:00<00:14,  1.62it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.47      0.46      0.47        59\n",
      "    is_treat       0.55      0.57      0.56        69\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.51      0.51      0.51       128\n",
      "weighted avg       0.51      0.52      0.51       128\n",
      "\n",
      "Testing DataLoader 0:   8%|▊         | 2/24 [00:01<00:12,  1.81it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.31      0.37      0.34        59\n",
      "    is_treat       0.36      0.30      0.33        69\n",
      "\n",
      "    accuracy                           0.34       128\n",
      "   macro avg       0.34      0.34      0.34       128\n",
      "weighted avg       0.34      0.34      0.34       128\n",
      "\n",
      "Testing DataLoader 0:  12%|█▎        | 3/24 [00:01<00:11,  1.84it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.49      0.45      0.47        64\n",
      "    is_treat       0.49      0.53      0.51        64\n",
      "\n",
      "    accuracy                           0.49       128\n",
      "   macro avg       0.49      0.49      0.49       128\n",
      "weighted avg       0.49      0.49      0.49       128\n",
      "\n",
      "Testing DataLoader 0:  17%|█▋        | 4/24 [00:02<00:10,  1.88it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.53      0.51      0.52        65\n",
      "    is_treat       0.52      0.54      0.53        63\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.52      0.52      0.52       128\n",
      "weighted avg       0.52      0.52      0.52       128\n",
      "\n",
      "Testing DataLoader 0:  21%|██        | 5/24 [00:02<00:09,  1.92it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.48      0.57      0.52        65\n",
      "    is_treat       0.45      0.37      0.40        63\n",
      "\n",
      "    accuracy                           0.47       128\n",
      "   macro avg       0.47      0.47      0.46       128\n",
      "weighted avg       0.47      0.47      0.46       128\n",
      "\n",
      "Testing DataLoader 0:  25%|██▌       | 6/24 [00:03<00:09,  1.95it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.47      0.56      0.51        63\n",
      "    is_treat       0.47      0.38      0.42        65\n",
      "\n",
      "    accuracy                           0.47       128\n",
      "   macro avg       0.47      0.47      0.47       128\n",
      "weighted avg       0.47      0.47      0.46       128\n",
      "\n",
      "Testing DataLoader 0:  29%|██▉       | 7/24 [00:03<00:09,  1.88it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.45      0.48      0.46        54\n",
      "    is_treat       0.60      0.57      0.58        74\n",
      "\n",
      "    accuracy                           0.53       128\n",
      "   macro avg       0.52      0.52      0.52       128\n",
      "weighted avg       0.54      0.53      0.53       128\n",
      "\n",
      "Testing DataLoader 0:  33%|███▎      | 8/24 [00:04<00:08,  1.84it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.41      0.44      0.42        57\n",
      "    is_treat       0.52      0.49      0.51        71\n",
      "\n",
      "    accuracy                           0.47       128\n",
      "   macro avg       0.47      0.47      0.47       128\n",
      "weighted avg       0.47      0.47      0.47       128\n",
      "\n",
      "Testing DataLoader 0:  38%|███▊      | 9/24 [00:04<00:08,  1.85it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.47      0.39      0.43        64\n",
      "    is_treat       0.48      0.56      0.52        64\n",
      "\n",
      "    accuracy                           0.48       128\n",
      "   macro avg       0.48      0.48      0.47       128\n",
      "weighted avg       0.48      0.48      0.47       128\n",
      "\n",
      "Testing DataLoader 0:  42%|████▏     | 10/24 [00:05<00:07,  1.87it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.37      0.45      0.41        62\n",
      "    is_treat       0.35      0.27      0.31        66\n",
      "\n",
      "    accuracy                           0.36       128\n",
      "   macro avg       0.36      0.36      0.36       128\n",
      "weighted avg       0.36      0.36      0.35       128\n",
      "\n",
      "Testing DataLoader 0:  46%|████▌     | 11/24 [00:05<00:07,  1.85it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.41      0.47      0.44        55\n",
      "    is_treat       0.55      0.48      0.51        73\n",
      "\n",
      "    accuracy                           0.48       128\n",
      "   macro avg       0.48      0.48      0.47       128\n",
      "weighted avg       0.49      0.48      0.48       128\n",
      "\n",
      "Testing DataLoader 0:  50%|█████     | 12/24 [00:06<00:06,  1.81it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.42      0.49      0.45        61\n",
      "    is_treat       0.45      0.37      0.41        67\n",
      "\n",
      "    accuracy                           0.43       128\n",
      "   macro avg       0.43      0.43      0.43       128\n",
      "weighted avg       0.43      0.43      0.43       128\n",
      "\n",
      "Testing DataLoader 0:  54%|█████▍    | 13/24 [00:07<00:06,  1.79it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.45      0.50      0.48        58\n",
      "    is_treat       0.55      0.50      0.52        70\n",
      "\n",
      "    accuracy                           0.50       128\n",
      "   macro avg       0.50      0.50      0.50       128\n",
      "weighted avg       0.50      0.50      0.50       128\n",
      "\n",
      "Testing DataLoader 0:  58%|█████▊    | 14/24 [00:07<00:05,  1.79it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.44      0.43      0.43        63\n",
      "    is_treat       0.45      0.46      0.46        65\n",
      "\n",
      "    accuracy                           0.45       128\n",
      "   macro avg       0.45      0.45      0.45       128\n",
      "weighted avg       0.45      0.45      0.45       128\n",
      "\n",
      "Testing DataLoader 0:  62%|██████▎   | 15/24 [00:08<00:05,  1.77it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.56      0.53      0.54        64\n",
      "    is_treat       0.55      0.58      0.56        64\n",
      "\n",
      "    accuracy                           0.55       128\n",
      "   macro avg       0.55      0.55      0.55       128\n",
      "weighted avg       0.55      0.55      0.55       128\n",
      "\n",
      "Testing DataLoader 0:  67%|██████▋   | 16/24 [00:09<00:04,  1.76it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.60      0.62      0.61        73\n",
      "    is_treat       0.47      0.45      0.46        55\n",
      "\n",
      "    accuracy                           0.55       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.55      0.55       128\n",
      "\n",
      "Testing DataLoader 0:  71%|███████   | 17/24 [00:09<00:03,  1.76it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.50      0.50      0.50        70\n",
      "    is_treat       0.40      0.40      0.40        58\n",
      "\n",
      "    accuracy                           0.45       128\n",
      "   macro avg       0.45      0.45      0.45       128\n",
      "weighted avg       0.45      0.45      0.45       128\n",
      "\n",
      "Testing DataLoader 0:  75%|███████▌  | 18/24 [00:10<00:03,  1.70it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.55      0.61      0.58        66\n",
      "    is_treat       0.53      0.47      0.50        62\n",
      "\n",
      "    accuracy                           0.54       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.54      0.54       128\n",
      "\n",
      "Testing DataLoader 0:  79%|███████▉  | 19/24 [00:11<00:02,  1.68it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.49      0.52      0.50        66\n",
      "    is_treat       0.46      0.44      0.45        62\n",
      "\n",
      "    accuracy                           0.48       128\n",
      "   macro avg       0.48      0.48      0.47       128\n",
      "weighted avg       0.48      0.48      0.48       128\n",
      "\n",
      "Testing DataLoader 0:  83%|████████▎ | 20/24 [00:12<00:02,  1.63it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.45      0.49      0.47        67\n",
      "    is_treat       0.38      0.34      0.36        61\n",
      "\n",
      "    accuracy                           0.42       128\n",
      "   macro avg       0.42      0.42      0.42       128\n",
      "weighted avg       0.42      0.42      0.42       128\n",
      "\n",
      "Testing DataLoader 0:  88%|████████▊ | 21/24 [00:13<00:01,  1.57it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.55      0.56      0.55        72\n",
      "    is_treat       0.42      0.41      0.41        56\n",
      "\n",
      "    accuracy                           0.49       128\n",
      "   macro avg       0.48      0.48      0.48       128\n",
      "weighted avg       0.49      0.49      0.49       128\n",
      "\n",
      "Testing DataLoader 0:  92%|█████████▏| 22/24 [00:14<00:01,  1.54it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.51      0.33      0.40        72\n",
      "    is_treat       0.41      0.59      0.48        56\n",
      "\n",
      "    accuracy                           0.45       128\n",
      "   macro avg       0.46      0.46      0.44       128\n",
      "weighted avg       0.47      0.45      0.44       128\n",
      "\n",
      "Testing DataLoader 0:  96%|█████████▌| 23/24 [00:14<00:00,  1.54it/s]TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    is_cause       0.50      0.49      0.50        73\n",
      "    is_treat       0.34      0.35      0.34        55\n",
      "\n",
      "    accuracy                           0.43       128\n",
      "   macro avg       0.42      0.42      0.42       128\n",
      "weighted avg       0.43      0.43      0.43       128\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 24/24 [00:15<00:00,  1.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_trainer.test(model, dataloaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing this milestone we have tried different architectures and ideas based on other papers in nlp however they all ended up being quite insuffucient at predicting our labels. We propose looking at more complex models like BERT for further improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tuwnlpie')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f40551e269d9f28521d93e6d718f9b57bce738cedf50cbce1242ce361fd269d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
