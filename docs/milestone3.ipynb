{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "usecols = ['food_entity', 'disease_entity', 'sentence', 'is_cause', 'is_treat']\n",
    "fd_df = pd.read_csv(\n",
    "    Path('../data/food_disease.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=usecols).rename({'food_entity': 'term1', 'disease_entity': 'term2'}, axis=1)\n",
    "fd_df['sentence'] = fd_df['sentence'].map(lambda x: x.lower())\n",
    "fd_df = fd_df[fd_df.apply(lambda x: x['term1'] in x['sentence'] and x['term2'] in x['sentence'], axis=1)]\n",
    "label_cols = ['is_cause', 'is_treat']\n",
    "fd_df['is_cause'] = fd_df['is_cause'].astype(float).astype(int)\n",
    "fd_df['is_treat'] = fd_df['is_treat'].astype(float).astype(int)\n",
    "# fd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cause = pd.read_csv(\n",
    "    Path('../data/crowd_truth_cause.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=['sentence', 'term1', 'term2']\n",
    ")\n",
    "df_cause['is_cause'] = 1\n",
    "df_cause['is_treat'] = 0\n",
    "df_treat = pd.read_csv(\n",
    "    Path('../data/crowd_truth_treat.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=['sentence', 'term1', 'term2']\n",
    ")\n",
    "df_treat['is_treat'] = 1\n",
    "df_treat['is_cause'] = 0\n",
    "ct_df = df_cause.append(df_treat, ignore_index=True)\n",
    "# ct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_df['source'] = 'FoodDisease'\n",
    "ct_df['source'] = 'CrowdTruth'\n",
    "df = pd.concat([fd_df, ct_df])\n",
    "\n",
    "term1 = 'term1'\n",
    "term2 = 'term2'\n",
    "\n",
    "df['prp_sent'] = df['sentence']\n",
    "df['prp_sent'] = df.apply(lambda x: x['prp_sent'].replace(x['term1'], term1), axis=1)\n",
    "df['prp_sent'] = df.apply(lambda x: x['prp_sent'].replace(x['term2'], term2), axis=1)\n",
    "df = df[df['prp_sent'].apply(lambda x: term1 in x and term2 in x)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy networkx\n",
    "import spacy\n",
    "import networkx as nx\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')    \n",
    "doc = nlp(df['sentence'].iloc[0])\n",
    "\n",
    "def shortest_dep_path(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((\n",
    "                '{0}'.format(token.lemma_),\n",
    "                '{0}'.format(child.lemma_)))\n",
    "    graph = nx.Graph(edges)\n",
    "    entity1 = term1\n",
    "    entity2 = term2\n",
    "    try:\n",
    "        return nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "df['sdp'] = df['prp_sent'].apply(shortest_dep_path)\n",
    "df = df[df['sdp'].apply(len) > 0]\n",
    "df['sdp_joined'] = df['sdp'].apply(lambda x: ' '.join(x))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Path('../data/milestone3/preprocessed.csv'), sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_cause</th>\n",
       "      <th>is_treat</th>\n",
       "      <th>source</th>\n",
       "      <th>prp_sent</th>\n",
       "      <th>sdp</th>\n",
       "      <th>sdp_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bombax ceiba l. (bombacaceae) fruits</td>\n",
       "      <td>urinary stones</td>\n",
       "      <td>interestingly, many indian tribes use bombax c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>FoodDisease</td>\n",
       "      <td>interestingly, many indian tribes use term1 as...</td>\n",
       "      <td>['term1', 'use', 'as', 'medicine', 'for', 'tre...</td>\n",
       "      <td>term1 use as medicine for treatment of term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ultra-processed food</td>\n",
       "      <td>obesity</td>\n",
       "      <td>ultra-processed food consumption has been asso...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FoodDisease</td>\n",
       "      <td>term1 consumption has been associated with sev...</td>\n",
       "      <td>['term1', 'consumption', 'associate', 'with', ...</td>\n",
       "      <td>term1 consumption associate with outcome as term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salt</td>\n",
       "      <td>osteoporosis</td>\n",
       "      <td>salt has notoriously been blamed for causing a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FoodDisease</td>\n",
       "      <td>term1 has notoriously been blamed for causing ...</td>\n",
       "      <td>['term1', 'blame', 'for', 'term2']</td>\n",
       "      <td>term1 blame for term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>broiler chicken</td>\n",
       "      <td>footpad dermatitis</td>\n",
       "      <td>in broiler chicken flocks when animal-based me...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FoodDisease</td>\n",
       "      <td>in term1 flocks when animal-based measures rel...</td>\n",
       "      <td>['term1', 'flock', 'in', 'identify', 'burn', '...</td>\n",
       "      <td>term1 flock in identify burn , term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sfp</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>in short, this study demonstrated that sfp cou...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>FoodDisease</td>\n",
       "      <td>in short, this study demonstrated that term1 c...</td>\n",
       "      <td>['term1', 'develop', 'as', 'food', 'or', 'prev...</td>\n",
       "      <td>term1 develop as food or prevention of term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>PARKINSON'S DISEASE</td>\n",
       "      <td>AMANTADINE</td>\n",
       "      <td>A 61 year old man with PARKINSON'S DISEASE (PD...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CrowdTruth</td>\n",
       "      <td>A 61 year old man with term1 (PD) developed su...</td>\n",
       "      <td>['term1', 'with', 'man', 'develop', 'after', '...</td>\n",
       "      <td>term1 with man develop after initiation of tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>DEPRESSION</td>\n",
       "      <td>IMIPRAMINE</td>\n",
       "      <td>With successful treatment of the patient's dep...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CrowdTruth</td>\n",
       "      <td>With successful treatment of the patient's dep...</td>\n",
       "      <td>['term1', 'of', 'treatment', 'with', 'term2']</td>\n",
       "      <td>term1 of treatment with term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>ANGI</td>\n",
       "      <td>BEPRIDIL</td>\n",
       "      <td>Five of 15 patients receiving bepridil did not...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CrowdTruth</td>\n",
       "      <td>Five of 15 patients receiving bepridil did not...</td>\n",
       "      <td>['term1', 'of', 'onset', 'to', 'minute', 'with...</td>\n",
       "      <td>term1 of onset to minute with term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>HEMOPHILIA A</td>\n",
       "      <td>FACTOR VIII</td>\n",
       "      <td>The development of antibodies to factor VIII i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CrowdTruth</td>\n",
       "      <td>The development of antibodies to factor VIII i...</td>\n",
       "      <td>['term1', 'with', 'protocol', 'combine', 'term2']</td>\n",
       "      <td>term1 with protocol combine term2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>FOOD STIMULATED GASTRO OESOPHAGEAL REFLUX MECH...</td>\n",
       "      <td>CISAPRIDE</td>\n",
       "      <td>The influence of CISAPRIDE on FOOD STIMULATED ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CrowdTruth</td>\n",
       "      <td>The influence of term2 on term1 was studied in...</td>\n",
       "      <td>['term1', 'on', 'influence', 'of', 'term2']</td>\n",
       "      <td>term1 on influence of term2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7978 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  term1               term2  \\\n",
       "0                  bombax ceiba l. (bombacaceae) fruits      urinary stones   \n",
       "1                                  ultra-processed food             obesity   \n",
       "2                                                  salt        osteoporosis   \n",
       "3                                       broiler chicken  footpad dermatitis   \n",
       "4                                                   sfp            diabetes   \n",
       "...                                                 ...                 ...   \n",
       "7973                                PARKINSON'S DISEASE          AMANTADINE   \n",
       "7974                                         DEPRESSION          IMIPRAMINE   \n",
       "7975                                               ANGI            BEPRIDIL   \n",
       "7976                                       HEMOPHILIA A         FACTOR VIII   \n",
       "7977  FOOD STIMULATED GASTRO OESOPHAGEAL REFLUX MECH...           CISAPRIDE   \n",
       "\n",
       "                                               sentence  is_cause  is_treat  \\\n",
       "0     interestingly, many indian tribes use bombax c...         0         1   \n",
       "1     ultra-processed food consumption has been asso...         1         0   \n",
       "2     salt has notoriously been blamed for causing a...         1         0   \n",
       "3     in broiler chicken flocks when animal-based me...         0         0   \n",
       "4     in short, this study demonstrated that sfp cou...         0         1   \n",
       "...                                                 ...       ...       ...   \n",
       "7973  A 61 year old man with PARKINSON'S DISEASE (PD...         0         1   \n",
       "7974  With successful treatment of the patient's dep...         0         1   \n",
       "7975  Five of 15 patients receiving bepridil did not...         0         1   \n",
       "7976  The development of antibodies to factor VIII i...         0         1   \n",
       "7977  The influence of CISAPRIDE on FOOD STIMULATED ...         0         1   \n",
       "\n",
       "           source                                           prp_sent  \\\n",
       "0     FoodDisease  interestingly, many indian tribes use term1 as...   \n",
       "1     FoodDisease  term1 consumption has been associated with sev...   \n",
       "2     FoodDisease  term1 has notoriously been blamed for causing ...   \n",
       "3     FoodDisease  in term1 flocks when animal-based measures rel...   \n",
       "4     FoodDisease  in short, this study demonstrated that term1 c...   \n",
       "...           ...                                                ...   \n",
       "7973   CrowdTruth  A 61 year old man with term1 (PD) developed su...   \n",
       "7974   CrowdTruth  With successful treatment of the patient's dep...   \n",
       "7975   CrowdTruth  Five of 15 patients receiving bepridil did not...   \n",
       "7976   CrowdTruth  The development of antibodies to factor VIII i...   \n",
       "7977   CrowdTruth  The influence of term2 on term1 was studied in...   \n",
       "\n",
       "                                                    sdp  \\\n",
       "0     ['term1', 'use', 'as', 'medicine', 'for', 'tre...   \n",
       "1     ['term1', 'consumption', 'associate', 'with', ...   \n",
       "2                    ['term1', 'blame', 'for', 'term2']   \n",
       "3     ['term1', 'flock', 'in', 'identify', 'burn', '...   \n",
       "4     ['term1', 'develop', 'as', 'food', 'or', 'prev...   \n",
       "...                                                 ...   \n",
       "7973  ['term1', 'with', 'man', 'develop', 'after', '...   \n",
       "7974      ['term1', 'of', 'treatment', 'with', 'term2']   \n",
       "7975  ['term1', 'of', 'onset', 'to', 'minute', 'with...   \n",
       "7976  ['term1', 'with', 'protocol', 'combine', 'term2']   \n",
       "7977        ['term1', 'on', 'influence', 'of', 'term2']   \n",
       "\n",
       "                                             sdp_joined  \n",
       "0          term1 use as medicine for treatment of term2  \n",
       "1     term1 consumption associate with outcome as term2  \n",
       "2                                 term1 blame for term2  \n",
       "3                  term1 flock in identify burn , term2  \n",
       "4          term1 develop as food or prevention of term2  \n",
       "...                                                 ...  \n",
       "7973  term1 with man develop after initiation of tre...  \n",
       "7974                      term1 of treatment with term2  \n",
       "7975                term1 of onset to minute with term2  \n",
       "7976                  term1 with protocol combine term2  \n",
       "7977                        term1 on influence of term2  \n",
       "\n",
       "[7978 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(Path('../data/milestone3/preprocessed.csv'), sep=';')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT features with traditional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sifel\\Miniconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers evaluate datasets\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "# Idea: use bert embeddings as features for SVC\n",
    "# Here using shortest dep path makes sense because it gives context between the 2 entities in question\n",
    "\n",
    "# https://github.com/EmilyAlsentzer/clinicalBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "pipe = pipeline('feature-extraction', model=model, tokenizer=tokenizer, binary_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "sent_embeddings = pipe(list(df['prp_sent']))\n",
    "sdp_embeddings = pipe(list(df['sdp_joined']))\n",
    "\n",
    "joblib.dump(Path('../data/milestone3/sent_embeddings.pkl'), sent_embeddings)\n",
    "joblib.dump(Path('../data/milestone3/sdp_embeddings.pkl'), sdp_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# sent_embeddings = joblib.load(Path('../data/milestone3/sent_embeddings.pkl'))\n",
    "sdp_embeddings = joblib.load(Path('../data/milestone3/sdp_embeddings.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7978\n",
      "1\n",
      "12\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(sdp_embeddings))\n",
    "print(len(sdp_embeddings[0]))\n",
    "print(len(sdp_embeddings[0][0]))\n",
    "print(len(sdp_embeddings[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use embedding of [CLS] token to get approximate sentence representation\n",
    "sent_cls_embeddings = [x[0][0] for x in sdp_embeddings]\n",
    "# sdp_cls_embeddings = [x[0][0] for x in sent_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT with classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\alexs/.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT\\snapshots\\41943bf7f983007123c758373c5246305cc536ec\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\alexs/.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT\\snapshots\\41943bf7f983007123c758373c5246305cc536ec\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\alexs/.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT\\snapshots\\41943bf7f983007123c758373c5246305cc536ec\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\alexs/.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT\\snapshots\\41943bf7f983007123c758373c5246305cc536ec\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\alexs/.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT\\snapshots\\41943bf7f983007123c758373c5246305cc536ec\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\alexs/.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT\\snapshots\\41943bf7f983007123c758373c5246305cc536ec\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: term1, sentence, term2, source, is_treat, prp_sent, is_cause, __index_level_0__. If term1, sentence, term2, source, is_treat, prp_sent, is_cause, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 0\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3213\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 374 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 32\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mlabels)\n\u001b[0;32m     24\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     25\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1748\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1749\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1750\u001b[0m \n\u001b[0;32m   1751\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1753\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\datasets\\arrow_dataset.py:2601\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2600\u001b[0m     \u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2601\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\n\u001b[0;32m   2602\u001b[0m         key,\n\u001b[0;32m   2603\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\datasets\\arrow_dataset.py:2585\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2583\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   2584\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2585\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   2586\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2587\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[0;32m   2588\u001b[0m )\n\u001b[0;32m   2589\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\datasets\\formatting\\formatting.py:588\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    587\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n\u001b[1;32m--> 588\u001b[0m     _check_valid_index_key(key, size)\n\u001b[0;32m    589\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alexs\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\datasets\\formatting\\formatting.py:531\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[1;34m(key, size)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[0;32m    530\u001b[0m     \u001b[39mif\u001b[39;00m (key \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m key \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[1;32m--> 531\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    532\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: Invalid key: 374 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/training\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset\n",
    "\n",
    "# TODO fix training pipeling (check compute_metrics, check dataset)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    num_labels=2, problem_type='multi_label_classification'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(output_dir='test_trainer', evaluation_strategy='epoch')\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d417bd2ab1d24874f40818c162e49543738a63150873a482918d17b30ebf17c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
