{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Load Dataset"
=======
    "# Milestone 2 - Relation Classification Neural Network Model"
>>>>>>> e978994feab13cbe29e9b4d7ac8c066b98d2d99d
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/72095722.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df_caus.append(df_treat, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_cause</th>\n",
       "      <th>is_treat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUTISM</td>\n",
       "      <td>TANTRUM</td>\n",
       "      <td>The limited data suggest that, in children wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLEEP PROBLEM</td>\n",
       "      <td>FAMILY STRESS</td>\n",
       "      <td>SLEEP PROBLEMs are associated with difficult b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CEREBELLAR ATAXIA</td>\n",
       "      <td>DYSFUNCTION OF THE CEREBELLUM</td>\n",
       "      <td>The term CEREBELLAR ATAXIA is employed to indi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CEREBELLAR DEGENERATION</td>\n",
       "      <td>CHRONIC ETHANOL ABUSE</td>\n",
       "      <td>Non hereditary causes of cerebellar degenerati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HEART PROBLEM</td>\n",
       "      <td>ARTHRITIS</td>\n",
       "      <td>The disorder can present with a migratory ture...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7963</th>\n",
       "      <td>PARKINSON'S DISEASE</td>\n",
       "      <td>AMANTADINE</td>\n",
       "      <td>A 61 year old man with PARKINSON'S DISEASE (PD...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7964</th>\n",
       "      <td>DEPRESSION</td>\n",
       "      <td>IMIPRAMINE</td>\n",
       "      <td>With successful treatment of the patient's dep...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>ANGI</td>\n",
       "      <td>BEPRIDIL</td>\n",
       "      <td>Five of 15 patients receiving bepridil did not...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>HEMOPHILIA A</td>\n",
       "      <td>FACTOR VIII</td>\n",
       "      <td>The development of antibodies to factor VIII i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7967</th>\n",
       "      <td>FOOD STIMULATED GASTRO OESOPHAGEAL REFLUX MECH...</td>\n",
       "      <td>CISAPRIDE</td>\n",
       "      <td>The influence of CISAPRIDE on FOOD STIMULATED ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7968 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  term1  \\\n",
       "0                                                AUTISM   \n",
       "1                                         SLEEP PROBLEM   \n",
       "2                                     CEREBELLAR ATAXIA   \n",
       "3                               CEREBELLAR DEGENERATION   \n",
       "4                                         HEART PROBLEM   \n",
       "...                                                 ...   \n",
       "7963                                PARKINSON'S DISEASE   \n",
       "7964                                         DEPRESSION   \n",
       "7965                                               ANGI   \n",
       "7966                                       HEMOPHILIA A   \n",
       "7967  FOOD STIMULATED GASTRO OESOPHAGEAL REFLUX MECH...   \n",
       "\n",
       "                              term2  \\\n",
       "0                           TANTRUM   \n",
       "1                     FAMILY STRESS   \n",
       "2     DYSFUNCTION OF THE CEREBELLUM   \n",
       "3             CHRONIC ETHANOL ABUSE   \n",
       "4                         ARTHRITIS   \n",
       "...                             ...   \n",
       "7963                     AMANTADINE   \n",
       "7964                     IMIPRAMINE   \n",
       "7965                       BEPRIDIL   \n",
       "7966                    FACTOR VIII   \n",
       "7967                      CISAPRIDE   \n",
       "\n",
       "                                               sentence  is_cause  is_treat  \n",
       "0     The limited data suggest that, in children wit...         1         0  \n",
       "1     SLEEP PROBLEMs are associated with difficult b...         1         0  \n",
       "2     The term CEREBELLAR ATAXIA is employed to indi...         1         0  \n",
       "3     Non hereditary causes of cerebellar degenerati...         1         0  \n",
       "4     The disorder can present with a migratory ture...         1         0  \n",
       "...                                                 ...       ...       ...  \n",
       "7963  A 61 year old man with PARKINSON'S DISEASE (PD...         0         1  \n",
       "7964  With successful treatment of the patient's dep...         0         1  \n",
       "7965  Five of 15 patients receiving bepridil did not...         0         1  \n",
       "7966  The development of antibodies to factor VIII i...         0         1  \n",
       "7967  The influence of CISAPRIDE on FOOD STIMULATED ...         0         1  \n",
       "\n",
       "[7968 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "usedcols = ['sentence', 'term1', 'term2']\n",
    "\n",
    "df_caus = pd.read_csv(\n",
    "    Path('..', 'data', 'crowd_truth_cause.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=usedcols\n",
    ")\n",
    "df_caus[\"is_cause\"] = 1\n",
    "df_caus[\"is_treat\"] = 0\n",
    "df_treat = pd.read_csv(\n",
    "    Path('..', 'data', 'crowd_truth_treat.csv'),\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=usedcols\n",
    ")\n",
    "df_treat[\"is_treat\"] = 1\n",
    "df_treat[\"is_cause\"] = 0\n",
    "df = df_caus.append(df_treat, ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the limited data suggest that, in children with mental retardation, TERM_ONE is associated with aggression, destruction of property, and TERM_TWOs.\n",
      "TERM_ONEs are associated with difficult behaviors and TERM_TWO, and are often a focus of clinical attention over and above the primary asd diagnosis.\n",
      "the term TERM_ONE is employed to indicate ataxia that is due to TERM_TWO\n",
      "non hereditary causes of TERM_ONE include TERM_TWO, paraneoplastic TERM_ONE, high altitude cerebral oedema, coeliac disease, normal pressure hydrocephalus and cerebellitis.\n",
      "the disorder can present with a migratory ture of TERM_TWO with many other features like TERM_ONEs, skin rash, gait abnormality and skin nodules.\n",
      "Number of docs: 7821\n"
     ]
    }
   ],
   "source": [
    "# Make case insensitive (no loss because emphasis on words does not play a role)\n",
    "df['sentence'] = df['sentence'].map(lambda x: x.lower())\n",
    "# Replace entities in sentence with placeholder tokens (may be useful for generalization when using n-grams)\n",
    "df['sentence'] = df.apply(lambda x: x['sentence'].replace(x['term1'].lower(), 'TERM_ONE'), axis=1)\n",
    "df['sentence'] = df.apply(lambda x: x['sentence'].replace(x['term2'].lower(), 'TERM_TWO'), axis=1)\n",
    "\n",
    "for i in range(5):\n",
    "    print(df['sentence'][i])\n",
    "\n",
    "df = df[df['sentence'].apply(lambda x: 'TERM_ONE' in x and 'TERM_TWO' in x)]\n",
    "\n",
    "print(f\"Number of docs: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/892057628.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_cause'] = df['is_cause'].astype(float).astype(int)\n",
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/892057628.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_treat'] = df['is_treat'].astype(float).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_cause</th>\n",
       "      <th>is_treat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_cause  is_treat\n",
       "0         1         0\n",
       "1         1         0\n",
       "2         1         0\n",
       "3         1         0\n",
       "4         1         0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to right dtype\n",
    "label_cols = ['is_cause', 'is_treat']\n",
    "df['is_cause'] = df['is_cause'].astype(float).astype(int)\n",
    "df['is_treat'] = df['is_treat'].astype(float).astype(int)\n",
    "df[label_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/holu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/holu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/3913636531.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['sentence'].apply(lambda x: nltk.word_tokenize(x))\n",
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/3913636531.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in nltk.corpus.stopwords.words('english') and len(token) > 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['limit', 'data', 'suggest', 'children', 'mental', 'retard', 'term_on', 'associ', 'aggress', 'destruct', 'properti', 'term_two']\n",
      "['term_on', 'associ', 'difficult', 'behavior', 'term_two', 'often', 'focu', 'clinic', 'attent', 'primari', 'asd', 'diagnosi']\n",
      "['term', 'term_on', 'employ', 'indic', 'ataxia', 'due', 'term_two']\n",
      "['non', 'hereditari', 'caus', 'term_on', 'includ', 'term_two', 'paraneoplast', 'term_on', 'high', 'altitud', 'cerebr', 'oedema', 'coeliac', 'diseas', 'normal', 'pressur', 'hydrocephalu', 'cerebel']\n",
      "['disord', 'present', 'migratori', 'ture', 'term_two', 'mani', 'featur', 'like', 'term_on', 'skin', 'rash', 'gait', 'abnorm', 'skin', 'nodul']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/3913636531.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_stem'] = df['tokens'].apply(lambda x: [porter.stem(token) for token in x])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # for tokanization\n",
    "nltk.download('stopwords') # for stopword removal\n",
    "\n",
    "# Tokenize the sentences\n",
    "df['tokens'] = df['sentence'].apply(lambda x: nltk.word_tokenize(x))\n",
    "# Remove stop words and tokens with length smaller than 2 (i.e. punctuations)\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in nltk.corpus.stopwords.words('english') and len(token) > 1])\n",
    "# Perform stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "df['tokens_stem'] = df['tokens'].apply(lambda x: [porter.stem(token) for token in x])\n",
    "for i in range(5):\n",
    "    print(df['tokens_stem'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/holu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/holu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['limited', 'data', 'suggest', 'child', 'mental', 'retardation', 'TERM_ONE', 'associated', 'aggression', 'destruction', 'property', 'TERM_TWOs']\n",
      "['TERM_ONEs', 'associated', 'difficult', 'behavior', 'TERM_TWO', 'often', 'focus', 'clinical', 'attention', 'primary', 'asd', 'diagnosis']\n",
      "['term', 'TERM_ONE', 'employed', 'indicate', 'ataxia', 'due', 'TERM_TWO']\n",
      "['non', 'hereditary', 'cause', 'TERM_ONE', 'include', 'TERM_TWO', 'paraneoplastic', 'TERM_ONE', 'high', 'altitude', 'cerebral', 'oedema', 'coeliac', 'disease', 'normal', 'pressure', 'hydrocephalus', 'cerebellitis']\n",
      "['disorder', 'present', 'migratory', 'ture', 'TERM_TWO', 'many', 'feature', 'like', 'TERM_ONEs', 'skin', 'rash', 'gait', 'abnormality', 'skin', 'nodule']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/t08nl7456zl9jcq07p2xt7x80000gn/T/ipykernel_11050/3900232636.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_lemma'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n"
     ]
    }
   ],
   "source": [
    "# Dependencies for WorNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['tokens_lemma'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "for i in range(5):\n",
    "    print(df['tokens_lemma'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time expression recognition\n",
    "standalone to python-time"
=======
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, matmul\n",
    "from torch.nn.functional import softmax\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
>>>>>>> e978994feab13cbe29e9b4d7ac8c066b98d2d99d
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load FoodDisease dataset preprocessed in Milestone 1\n",
    "feature_cols = ['food_entity', 'disease_entity', 'sentence', 'is_cause', 'is_treat', 'tokens', 'tokens_stem', 'tokens_lemma', 'sdp_tokens_lemma']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../data/processed/df.csv\",\n",
    "    sep=',', quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    usecols=feature_cols)\n",
    "\n",
    "feature_cols = ['food_entity', 'disease_entity', 'sentence', 'tokens', 'tokens_stem', 'tokens_lemma', 'sdp_tokens_lemma']\n",
    "label_cols = ['is_cause', 'is_treat']\n",
    "\n",
    "y = df[label_cols].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import copy\n",
    "\n",
    "df_x = df[\"sdp_tokens_lemma\"]\n",
    "\n",
    "X_tmp = copy.deepcopy(df_x)\n",
    "embedded_sentences = [one_hot(sent, 10000) for sent in X_tmp]\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(df_x, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "print(length_long_sentence)\n",
    "# will put length of longest sentence +1 bc 31 is not divisible by anything,\n",
    "# this is important for the number of heads in the transformer\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence+1, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5645 4369 7744 ...    0    0    0]\n",
      " [5645 4369 5907 ...    0    0    0]\n",
      " [5645 4369  695 ...    0    0    0]\n",
      " ...\n",
      " [5645 4369 3523 ...    0    0    0]\n",
      " [5645 4369 7324 ...    0    0    0]\n",
      " [5645 4369 7420 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475, 32)\n",
      "(60, 32)\n",
      "(60, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sentences,y,  test_size=0.1, random_state=1)\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train, y_train, test_size=0.111, random_state=1) # 0.111 x 0.9 = 0.1\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def get_x(self):\n",
    "        return self.x\n",
    "\n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return (torch.tensor(x).float(), torch.tensor(y).long())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def get_dataloader(self, batch_size=128, num_workers=0, shuffle=False):\n",
    "        return DataLoader(self, batch_size=batch_size, drop_last=True, pin_memory=True, num_workers=num_workers, shuffle=shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = TorchDataset(X_train,y_train)\n",
    "X_test = TorchDataset(X_test, y_test)\n",
    "X_val = TorchDataset(X_val, y_val)\n",
    "\n",
    "dataloaders = { 'train': X_train.get_dataloader(batch_size=length_long_sentence+1, shuffle=True), 'test': X_test.get_dataloader(batch_size=length_long_sentence+1, shuffle=False), 'val': X_val.get_dataloader(batch_size=length_long_sentence+1, shuffle=False) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(query, key, value, mask=None):\n",
    "        dk = query.size()[-1]\n",
    "        scores = query.matmul(key.transpose(-2, -1)) / math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        values = torch.matmul(attention, value)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        output = self.o_proj(values)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very unnecessary but its here, taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(LightningModule):\n",
    "    \n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr=1e-4, dropout=0.0, input_dropout=0.0, weight_decay=1e-2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Hidden dimensionality of the input\n",
    "            model_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout - Dropout to apply inside the model\n",
    "            input_dropout - Dropout to apply on the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout),\n",
    "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
    "                                              input_dim=self.hparams.model_dim,\n",
    "                                              dim_feedforward=2*self.hparams.model_dim,\n",
    "                                              num_heads=self.hparams.num_heads,\n",
    "                                              dropout=self.hparams.dropout)\n",
    "        # Output classifier per sequence lement\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.input_net(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "            return torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay)\n",
    "\n",
    "    def _loss_fn(self, out, y):\n",
    "        y = y.unsqueeze(0)\n",
    "        out = out.float()\n",
    "        y = y.float()\n",
    "        loss = nn.BCEWithLogitsLoss()(out, y) # Multiclass classification\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self._loss_fn(out, y)\n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self._loss_fn(out, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        accuracy = Accuracy()\n",
    "        acc = accuracy(out[0], y)\n",
    "        self.log('accuracy', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self._loss_fn(out, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        accuracy = Accuracy()\n",
    "        acc = accuracy(out[0], y)\n",
    "        self.log('accuracy', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTrainer():\n",
    "    def __init__(self, model, name, dirpath, dataloaders, max_epochs=50) -> None:\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.dirpath = dirpath\n",
    "        self.max_epochs = max_epochs\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def run(self):\n",
    "        logger = TensorBoardLogger(f\"{self.dirpath}/tensorboard\", name=self.name)\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(dirpath=Path(self.dirpath, self.name), monitor=\"val_loss\"),\n",
    "            EarlyStopping(monitor='loss')]\n",
    "        trainer = Trainer(deterministic=True, logger=logger, callbacks=callbacks, max_epochs=self.max_epochs)\n",
    "        trainer.fit(self.model, self.dataloaders['train'], self.dataloaders['val'])\n",
    "        val_result = trainer.test(self.model, self.dataloaders['val'], verbose=True)\n",
    "        test_result = trainer.test(self.model, self.dataloaders['test'], verbose=True)\n",
    "        result = {\"test_acc\": test_result, \"val_acc\": val_result}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "model = TransformerPredictor(input_dim=length_long_sentence+1, model_dim=length_long_sentence+1, num_layers=2, num_heads=8, num_classes=2)\n",
    "\n",
    "trainer = TorchTrainer(model, 'test', './', dataloaders, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Ana\\Desktop\\FER\\dip\\tu_wien\\nlp\\project-1div7\\docs\\test exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Sequential         | 1.1 K \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 17.1 K\n",
      "3 | output_net          | Sequential         | 1.2 K \n",
      "-----------------------------------------------------------\n",
      "19.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.3 K    Total params\n",
      "0.077     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1558: PossibleUserWarning: The number of training batches (14) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 26.05it/s, loss=0.605, v_num=77, loss_step=0.632, val_loss=0.612, accuracy=0.625, loss_epoch=0.607]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\tuwnlpie\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 22.73it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        accuracy                   0.625\n",
      "        val_loss            0.6115018129348755\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.26it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        accuracy                 0.609375\n",
      "        val_loss            0.6159040927886963\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "result = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'val_loss': 0.6115018129348755, 'accuracy': 0.625}]\n",
      "[{'val_loss': 0.6159040927886963, 'accuracy': 0.609375}]\n"
     ]
    }
   ],
   "source": [
    "print(result['val_acc'])\n",
    "print(result['test_acc'])"
   ]
>>>>>>> e978994feab13cbe29e9b4d7ac8c066b98d2d99d
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3.10.8 ('nlp')",
=======
   "display_name": "Python 3.10.8 ('tuwnlpie')",
>>>>>>> e978994feab13cbe29e9b4d7ac8c066b98d2d99d
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
<<<<<<< HEAD
    "hash": "3efb5a2bd22a1b5f50f0c388520d8d3e60e70db335b0771969a2b55381d10c7a"
=======
    "hash": "f40551e269d9f28521d93e6d718f9b57bce738cedf50cbce1242ce361fd269d1"
>>>>>>> e978994feab13cbe29e9b4d7ac8c066b98d2d99d
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
